{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-05T08:37:32.472049Z",
     "iopub.status.busy": "2025-02-05T08:37:32.471687Z",
     "iopub.status.idle": "2025-02-05T08:37:38.565303Z",
     "shell.execute_reply": "2025-02-05T08:37:38.564130Z",
     "shell.execute_reply.started": "2025-02-05T08:37:32.472018Z"
    },
    "papermill": {
     "duration": 5.054747,
     "end_time": "2025-02-02T16:36:16.384795",
     "exception": false,
     "start_time": "2025-02-02T16:36:11.330048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.10)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.9.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.12.14)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch-geometric) (2024.2.0)\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-geometric\n",
      "Successfully installed torch-geometric-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T08:37:38.567082Z",
     "iopub.status.busy": "2025-02-05T08:37:38.566710Z",
     "iopub.status.idle": "2025-02-05T08:38:58.347181Z",
     "shell.execute_reply": "2025-02-05T08:38:58.346244Z",
     "shell.execute_reply.started": "2025-02-05T08:37:38.567047Z"
    },
    "papermill": {
     "duration": 72.012154,
     "end_time": "2025-02-02T16:37:28.399991",
     "exception": false,
     "start_time": "2025-02-02T16:36:16.387837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-c90b9d14e3c5>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample 0 as prepared_samples/sample_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-c90b9d14e3c5>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample 10000 as prepared_samples/sample_10000.pt\n",
      "Dataset preparation completed.\n"
     ]
    }
   ],
   "source": [
    "#create samples :\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def load_graph(path, is_pickle=True):\n",
    "    \"\"\"\n",
    "    Load a molecule graph (.pkl) or a protein graph (.pt).\n",
    "    If is_pickle is True, use pickle to load the file; otherwise, use torch.load.\n",
    "    \"\"\"\n",
    "    if is_pickle:\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return torch.load(path)\n",
    "\n",
    "def prepare_dataset_individual_save_as_pt(filtered_dataset, molecule_graph_dir, protein_graph_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Incrementally prepares the dataset and saves each (molecule, protein, target) tuple as a separate .pt file.\n",
    "\n",
    "    Args:\n",
    "    - filtered_dataset: The filtered KIBA dataset (DataFrame).\n",
    "    - molecule_graph_dir: Directory where molecule graphs are stored.\n",
    "    - protein_graph_dir: Directory where protein graphs are stored.\n",
    "    - output_dir: Directory to save the prepared dataset incrementally.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for index, row in filtered_dataset.iterrows():\n",
    "        protein_id = row['Target_ID']\n",
    "        chembl_id = row['Drug_ID']\n",
    "\n",
    "        # Load the protein graph (.pt)\n",
    "        pro_graph_path = os.path.join(protein_graph_dir, f\"{protein_id}_graph.pt\")\n",
    "        if not os.path.exists(pro_graph_path):\n",
    "            print(f\"Protein graph not found: {protein_id}\")\n",
    "            continue\n",
    "        pro_graph = load_graph(pro_graph_path, is_pickle=False)\n",
    "\n",
    "        # Load the molecule graph (.pkl)\n",
    "        mol_graph_path = os.path.join(molecule_graph_dir, f\"{chembl_id}_graph.pkl\")\n",
    "        if not os.path.exists(mol_graph_path):\n",
    "            print(f\"Molecule graph not found: {chembl_id}\")\n",
    "            continue\n",
    "        mol_graph = load_graph(mol_graph_path)\n",
    "\n",
    "        # Load target (affinity value)\n",
    "        target = torch.tensor([row['Y']], dtype=torch.float)\n",
    "\n",
    "        # Create the sample as a tuple (molecule graph, protein graph, target)\n",
    "        sample = (mol_graph, pro_graph, target)\n",
    "\n",
    "        # Save the sample as a .pt file\n",
    "        sample_path = os.path.join(output_dir, f\"sample_{index}.pt\")\n",
    "        torch.save(sample, sample_path)\n",
    "\n",
    "        if(index%10000 == 0 ):\n",
    "            print(f\"Saved sample {index} as {sample_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage for individual saving\n",
    "molecule_graph_dir = '/kaggle/input/graphs/molecule_graphs'  # Directory where molecule graphs are stored\n",
    "protein_graph_dir = '/kaggle/input/graphs/ProteinGraphs'  # Directory where protein graphs are stored\n",
    "filtered_dataset_path = '/kaggle/input/graphs/filtered_DavisDataSet.csv'  # Path to the filtered dataset CSV\n",
    "output_dir = 'prepared_samples/'  # Directory to save individual samples\n",
    "\n",
    "# Load filtered dataset CSV\n",
    "filtered_dataset = pd.read_csv(filtered_dataset_path)\n",
    "\n",
    "# Prepare the dataset incrementally, saving each sample as a .pt file\n",
    "prepare_dataset_individual_save_as_pt(filtered_dataset, molecule_graph_dir, protein_graph_dir, output_dir)\n",
    "\n",
    "print(\"Dataset preparation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T08:38:58.349168Z",
     "iopub.status.busy": "2025-02-05T08:38:58.348723Z"
    },
    "papermill": {
     "duration": 15070.892141,
     "end_time": "2025-02-02T20:48:39.294980",
     "exception": false,
     "start_time": "2025-02-02T16:37:28.402839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda.\n",
      "Checkpoints will be saved to: /kaggle/working/TrainingModel1\n",
      "Starting fresh metrics tracking.\n",
      "GNNNet Loaded\n",
      "No existing checkpoint found; starting fresh.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/250 [00:51<?, ?epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/250] Training Loss: 1.1407\n"
     ]
    }
   ],
   "source": [
    "#model 1\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool as gep\n",
    "from torch_geometric.data import Data, Batch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Optional, for plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##############################################################################\n",
    "#                               1. METRICS\n",
    "##############################################################################\n",
    "\n",
    "@torch.no_grad()\n",
    "def ci_vectorized(preds: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Naive O(N^2) Concordance Index using vectorized GPU operations.\n",
    "    preds, targets: (N,) on the same device (e.g. cuda).\n",
    "    \"\"\"\n",
    "    # (N, N) differences\n",
    "    p_diff = preds.unsqueeze(1) - preds.unsqueeze(0)\n",
    "    t_diff = targets.unsqueeze(1) - targets.unsqueeze(0)\n",
    "    # mask out pairs where targets are identical\n",
    "    mask = (t_diff != 0)\n",
    "    # sign of product => +1 (concordant), 0 (tie), -1 (discordant)\n",
    "    sign_mat = torch.sign(p_diff * t_diff)\n",
    "    # step function h(x): 1 if x>0, 0.5 if x=0, 0 if x<0\n",
    "    h = (sign_mat == 1).float() + 0.5 * (sign_mat == 0).float()\n",
    "    # apply mask\n",
    "    h_masked = h * mask.float()\n",
    "    c = h_masked.sum()\n",
    "    s = mask.sum().float()\n",
    "    return (c / s).item() if s > 0 else 0.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def mse_torch(preds: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    GPU-friendly MSE using torch's built-in mean squared error.\n",
    "    \"\"\"\n",
    "    return F.mse_loss(preds, targets, reduction='mean').item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def pearson_torch(preds: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Pearson correlation coefficient in PyTorch (GPU-friendly).\n",
    "    preds, targets: (N,) on the same device\n",
    "    \"\"\"\n",
    "    p_centered = preds - preds.mean()\n",
    "    t_centered = targets - targets.mean()\n",
    "    cov = (p_centered * t_centered).sum()\n",
    "    denom = torch.sqrt((p_centered**2).sum()) * torch.sqrt((t_centered**2).sum())\n",
    "    eps = 1e-8\n",
    "    return (cov / (denom + eps)).item()\n",
    "\n",
    "##############################################################################\n",
    "#                       2. GNN MODEL DEFINITION\n",
    "##############################################################################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool as gep\n",
    "\n",
    "#############################################\n",
    "# Bidirectional Cross-Attention Module\n",
    "#############################################\n",
    "# class BidirectionalCrossAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim=128, num_heads=4):\n",
    "#         super().__init__()\n",
    "#         \"\"\"\n",
    "#         Bidirectional Cross-Attention Formulas:\n",
    "        \n",
    "#         For Graph A → Graph B:\n",
    "#             Q_A = X_A W_Q_A, K_B = X_B W_K_A, V_B = X_B W_V_A\n",
    "#             Z_A = softmax(Q_A K_B^T / √d) V_B\n",
    "            \n",
    "#         For Graph B → Graph A:\n",
    "#             Q_B = X_B W_Q_B, K_A = X_A W_K_B, V_A = X_A W_V_B\n",
    "#             Z_B = softmax(Q_B K_A^T / √d) V_A\n",
    "#         \"\"\"\n",
    "#         self.embed_dim = embed_dim\n",
    "        \n",
    "#         # Graph A → Graph B weights\n",
    "#         self.W_Q_A = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.W_K_A = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.W_V_A = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "#         # Graph B → Graph A weights\n",
    "#         self.W_Q_B = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.W_K_B = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.W_V_B = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "#         self.scale = 1.0 / torch.sqrt(torch.tensor(embed_dim, dtype=torch.float32))\n",
    "\n",
    "#     def forward(self, X_A, X_B, batch_A, batch_B):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             X_A: Graph A node embeddings [total_A_nodes, embed_dim]\n",
    "#             X_B: Graph B node embeddings [total_B_nodes, embed_dim]\n",
    "#             batch_A: Batch indices for Graph A [total_A_nodes]\n",
    "#             batch_B: Batch indices for Graph B [total_B_nodes]\n",
    "#         Returns:\n",
    "#             Z_A: Updated embeddings for Graph A (same shape as X_A)\n",
    "#             Z_B: Updated embeddings for Graph B (same shape as X_B)\n",
    "#         \"\"\"\n",
    "#         # --- Graph A updates from Graph B ---\n",
    "#         Q_A = self.W_Q_A(X_A)  # [N_A, embed_dim]\n",
    "#         K_B = self.W_K_A(X_B)  # [N_B, embed_dim]\n",
    "#         V_B = self.W_V_A(X_B)  # [N_B, embed_dim]\n",
    "        \n",
    "#         scores_AB = (Q_A @ K_B.T) * self.scale  # [N_A, N_B]\n",
    "#         mask_AB = self._create_mask(batch_A, batch_B)  # [N_A, N_B]\n",
    "#         scores_AB = scores_AB.masked_fill(~mask_AB, -1e9)\n",
    "#         attn_AB = F.softmax(scores_AB, dim=-1)\n",
    "#         Z_A = attn_AB @ V_B  # Updated Graph A features\n",
    "        \n",
    "#         # --- Graph B updates from Graph A ---\n",
    "#         Q_B = self.W_Q_B(X_B)  # [N_B, embed_dim]\n",
    "#         K_A = self.W_K_B(X_A)  # [N_A, embed_dim]\n",
    "#         V_A = self.W_V_B(X_A)  # [N_A, embed_dim]\n",
    "        \n",
    "#         scores_BA = (Q_B @ K_A.T) * self.scale  # [N_B, N_A]\n",
    "#         mask_BA = mask_AB.T  # [N_B, N_A]\n",
    "#         scores_BA = scores_BA.masked_fill(~mask_BA, -1e9)\n",
    "#         attn_BA = F.softmax(scores_BA, dim=-1)\n",
    "#         Z_B = attn_BA @ V_A  # Updated Graph B features\n",
    "        \n",
    "#         return Z_A, Z_B\n",
    "\n",
    "#     def _create_mask(self, batch_A, batch_B):\n",
    "#         # Creates a mask [N_A, N_B] so that nodes only attend to nodes from the same sample.\n",
    "#         return batch_A.unsqueeze(1) == batch_B.unsqueeze(0)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BidirectionalCrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=4, chunk_size=2048):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Bidirectional Cross-Attention Formulas:\n",
    "        \n",
    "        For Graph A → Graph B:\n",
    "            Q_A = X_A W_Q_A, K_B = X_B W_K_A, V_B = X_B W_V_A\n",
    "            Z_A = softmax(Q_A K_B^T / √d) V_B\n",
    "            \n",
    "        For Graph B → Graph A:\n",
    "            Q_B = X_B W_Q_B, K_A = X_A W_K_B, V_A = X_A W_V_B\n",
    "            Z_B = softmax(Q_B K_A^T / √d) V_A\n",
    "        \"\"\"\n",
    "        self.embed_dim = embed_dim\n",
    "        self.chunk_size = chunk_size  # Adjust this value to control memory usage\n",
    "        \n",
    "        # Graph A → Graph B weights\n",
    "        self.W_Q_A = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_K_A = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_V_A = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Graph B → Graph A weights\n",
    "        self.W_Q_B = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_K_B = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_V_B = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.scale = 1.0 / torch.sqrt(torch.tensor(embed_dim, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, X_A, X_B, batch_A, batch_B):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_A: Graph A node embeddings [total_A_nodes, embed_dim]\n",
    "            X_B: Graph B node embeddings [total_B_nodes, embed_dim]\n",
    "            batch_A: Batch indices for Graph A [total_A_nodes]\n",
    "            batch_B: Batch indices for Graph B [total_B_nodes]\n",
    "        Returns:\n",
    "            Z_A: Updated embeddings for Graph A (same shape as X_A)\n",
    "            Z_B: Updated embeddings for Graph B (same shape as X_B)\n",
    "        \"\"\"\n",
    "        # Compute linear projections for both directions\n",
    "        Q_A = self.W_Q_A(X_A)  # [N_A, embed_dim]\n",
    "        K_B = self.W_K_A(X_B)  # [N_B, embed_dim]\n",
    "        V_B = self.W_V_A(X_B)  # [N_B, embed_dim]\n",
    "        \n",
    "        Q_B = self.W_Q_B(X_B)  # [N_B, embed_dim]\n",
    "        K_A = self.W_K_B(X_A)  # [N_A, embed_dim]\n",
    "        V_A = self.W_V_B(X_A)  # [N_A, embed_dim]\n",
    "        \n",
    "        # Create mask so that only nodes from the same sample attend to each other\n",
    "        mask_AB = self._create_mask(batch_A, batch_B)  # [N_A, N_B]\n",
    "        mask_BA = mask_AB.T  # [N_B, N_A]\n",
    "        \n",
    "        # Compute Z_A in chunks to reduce memory usage\n",
    "        Z_A_chunks = []\n",
    "        for i in range(0, Q_A.size(0), self.chunk_size):\n",
    "            Q_A_chunk = Q_A[i:i+self.chunk_size]  # [chunk_size, embed_dim]\n",
    "            # Compute attention scores for this chunk: [chunk_size, N_B]\n",
    "            scores_chunk = (Q_A_chunk @ K_B.T) * self.scale\n",
    "            mask_chunk = mask_AB[i:i+self.chunk_size]  # [chunk_size, N_B]\n",
    "            scores_chunk = scores_chunk.masked_fill(~mask_chunk, -1e9)\n",
    "            attn_chunk = F.softmax(scores_chunk, dim=-1)\n",
    "            Z_A_chunk = attn_chunk @ V_B  # [chunk_size, embed_dim]\n",
    "            Z_A_chunks.append(Z_A_chunk)\n",
    "        Z_A = torch.cat(Z_A_chunks, dim=0)\n",
    "        \n",
    "        # Similarly, compute Z_B in chunks along the query dimension of Graph B\n",
    "        Z_B_chunks = []\n",
    "        for i in range(0, Q_B.size(0), self.chunk_size):\n",
    "            Q_B_chunk = Q_B[i:i+self.chunk_size]  # [chunk_size, embed_dim]\n",
    "            scores_chunk = (Q_B_chunk @ K_A.T) * self.scale  # [chunk_size, N_A]\n",
    "            mask_chunk = mask_BA[i:i+self.chunk_size]  # [chunk_size, N_A]\n",
    "            scores_chunk = scores_chunk.masked_fill(~mask_chunk, -1e9)\n",
    "            attn_chunk = F.softmax(scores_chunk, dim=-1)\n",
    "            Z_B_chunk = attn_chunk @ V_A  # [chunk_size, embed_dim]\n",
    "            Z_B_chunks.append(Z_B_chunk)\n",
    "        Z_B = torch.cat(Z_B_chunks, dim=0)\n",
    "        \n",
    "        return Z_A, Z_B\n",
    "\n",
    "    def _create_mask(self, batch_A, batch_B):\n",
    "        \"\"\"Creates a mask [N_A, N_B] so that nodes only attend to nodes from the same sample.\"\"\"\n",
    "        return batch_A.unsqueeze(1) == batch_B.unsqueeze(0)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Updated GNN Model with Cross-Attention (No Extra Projection)\n",
    "#############################################\n",
    "class GNNNet(torch.nn.Module):\n",
    "    def __init__(self, n_output=1, num_features_pro=54, num_features_mol=78, output_dim=128, dropout=0.2):\n",
    "        super(GNNNet, self).__init__()\n",
    "        print('GNNNet Loaded')\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        # --- Molecule branch GCN layers ---\n",
    "        self.mol_conv1 = GCNConv(num_features_mol, num_features_mol)\n",
    "        # Set GCN2 to output 'output_dim' so that both branches share the same dimension.\n",
    "        self.mol_conv2 = GCNConv(num_features_mol, 156)\n",
    "        # GCN3 will process the updated features; input dim = output_dim.\n",
    "        self.mol_conv3 = GCNConv(156, num_features_mol*4)\n",
    "        \n",
    "        self.mol_fc_g1 = nn.Linear(num_features_mol*4, 1024)\n",
    "        self.mol_fc_g2 = nn.Linear(1024, output_dim)\n",
    "        \n",
    "        # --- Protein branch GCN layers ---\n",
    "        self.pro_conv1 = GCNConv(num_features_pro, num_features_pro)\n",
    "        self.pro_conv2 = GCNConv(num_features_pro, 156)\n",
    "        self.pro_conv3 = GCNConv(156, num_features_pro*4)\n",
    "        \n",
    "        self.pro_fc_g1 = nn.Linear(num_features_pro*4, 1024)\n",
    "        self.pro_fc_g2 = nn.Linear(1024, output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # --- Bidirectional Cross-Attention ---\n",
    "        # Now the node features from both branches are already in R^(output_dim)\n",
    "        self.cross_attn = BidirectionalCrossAttention(embed_dim=156, num_heads=4)\n",
    "        \n",
    "        # --- Combined FC layers ---\n",
    "        self.fc1 = nn.Linear(2 * output_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.out = nn.Linear(512, self.n_output)\n",
    "\n",
    "    def forward(self, data_mol, data_pro):\n",
    "        # Unpack molecule graph data\n",
    "        mol_x, mol_edge_index, mol_batch = data_mol.x, data_mol.edge_index, data_mol.batch\n",
    "        # Unpack protein graph data\n",
    "        target_x, target_edge_index, target_batch = data_pro.x, data_pro.edge_index, data_pro.batch\n",
    "\n",
    "        # --- Molecule branch: GCN1 -> GCN2 ---\n",
    "        x = self.mol_conv1(mol_x, mol_edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.mol_conv2(x, mol_edge_index)\n",
    "        x = self.relu(x)\n",
    "        # x now has shape: [total_mol_nodes, output_dim]\n",
    "        \n",
    "        # --- Protein branch: GCN1 -> GCN2 ---\n",
    "        xt = self.pro_conv1(target_x, target_edge_index)\n",
    "        xt = self.relu(xt)\n",
    "        xt = self.pro_conv2(xt, target_edge_index)\n",
    "        xt = self.relu(xt)\n",
    "        # xt now has shape: [total_pro_nodes, output_dim]\n",
    "        \n",
    "        # --- Apply bidirectional cross-attention ---\n",
    "        # The cross-attention module uses batch indices to ensure nodes only attend to nodes from the same sample.\n",
    "        Xnew, XtNew = self.cross_attn(x, xt, mol_batch, target_batch)\n",
    "        \n",
    "        # --- Continue with third GCN layer using the updated features ---\n",
    "        x_updated = self.mol_conv3(Xnew, mol_edge_index)\n",
    "        x_updated = self.relu(x_updated)\n",
    "        x_pooled = gep(x_updated, mol_batch)  # Global pooling for molecule graphs\n",
    "        \n",
    "        xt_updated = self.pro_conv3(XtNew, target_edge_index)\n",
    "        xt_updated = self.relu(xt_updated)\n",
    "        xt_pooled = gep(xt_updated, target_batch)  # Global pooling for protein graphs\n",
    "        \n",
    "        # --- Flatten branch outputs through FC layers ---\n",
    "        x_final = self.mol_fc_g2(self.relu(self.mol_fc_g1(x_pooled)))\n",
    "        x_final = self.dropout(x_final)\n",
    "        xt_final = self.pro_fc_g2(self.relu(self.pro_fc_g1(xt_pooled)))\n",
    "        xt_final = self.dropout(xt_final)\n",
    "        \n",
    "        # --- Concatenate and apply combined FC layers ---\n",
    "        xc = torch.cat((x_final, xt_final), dim=1)\n",
    "        xc = self.fc1(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        xc = self.fc2(xc)\n",
    "        xc = self.relu(xc)\n",
    "        xc = self.dropout(xc)\n",
    "        out = self.out(xc)\n",
    "        \n",
    "        return out\n",
    "\n",
    "##############################################################################\n",
    "#                   3. DATA LOADING HELPERS\n",
    "##############################################################################\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "def load_sample(path):\n",
    "    \"\"\"\n",
    "    Load a .pt sample, fix up 'x' and 'edge_index', return (mol_data, pro_data, target).\n",
    "    \"\"\"\n",
    "    sample = torch.load(path)\n",
    "    mol_data, pro_data, target = sample[0], sample[1], sample[2]\n",
    "\n",
    "    # Convert dict to Data if needed\n",
    "    if isinstance(mol_data, dict):\n",
    "        mol_data = Data(**mol_data)\n",
    "    if isinstance(pro_data, dict):\n",
    "        pro_data = Data(**pro_data)\n",
    "\n",
    "    # Fix x\n",
    "    if not hasattr(mol_data, 'x') or mol_data.x is None:\n",
    "        if hasattr(mol_data, 'features'):\n",
    "            mol_data.x = mol_data.features\n",
    "            del mol_data.features\n",
    "        else:\n",
    "            raise ValueError(\"mol_data missing 'x' or 'features'\")\n",
    "    if not hasattr(pro_data, 'x') or pro_data.x is None:\n",
    "        if hasattr(pro_data, 'features'):\n",
    "            pro_data.x = pro_data.features\n",
    "            del pro_data.features\n",
    "        else:\n",
    "            raise ValueError(\"pro_data missing 'x' or 'features'\")\n",
    "\n",
    "    mol_data.x = torch.as_tensor(mol_data.x, dtype=torch.float32)\n",
    "    pro_data.x = torch.as_tensor(pro_data.x, dtype=torch.float32)\n",
    "\n",
    "    # Fix edge_index\n",
    "    def fix_edge_index(d):\n",
    "        if not isinstance(d.edge_index, torch.Tensor):\n",
    "            d.edge_index = torch.tensor(d.edge_index, dtype=torch.long)\n",
    "        else:\n",
    "            d.edge_index = d.edge_index.long()\n",
    "        if d.edge_index.shape[0] != 2:\n",
    "            d.edge_index = d.edge_index.t()\n",
    "        d.num_nodes = d.x.size(0)\n",
    "\n",
    "    fix_edge_index(mol_data)\n",
    "    fix_edge_index(pro_data)\n",
    "\n",
    "    return (mol_data, pro_data, target)\n",
    "\n",
    "def batch_loader(file_list, sample_dir, batch_size):\n",
    "    \"\"\"\n",
    "    Yields batches of (mol_data, pro_data, target) from file_list.\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "    for file_name in file_list:\n",
    "        path = os.path.join(sample_dir, file_name)\n",
    "        sample = load_sample(path)\n",
    "        batch.append(sample)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "##############################################################################\n",
    "#              4. TRAINING / EVALUATION WITH METRICS EACH EPOCH\n",
    "##############################################################################\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def train_and_evaluate(sample_dir, num_epochs=10, test_size=0.2, lr=0.001):\n",
    "    \"\"\"\n",
    "    Trains the GNN model, evaluates on train & test each epoch, saves metrics + checkpoints.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Running on {device}.\")\n",
    "\n",
    "    # Gather samples\n",
    "    sample_files = [f for f in os.listdir(sample_dir) if f.endswith('.pt')]\n",
    "    assert len(sample_files) > 0, \"No .pt files found in sample_dir!\"\n",
    "\n",
    "    # Split\n",
    "    train_files, test_files = train_test_split(sample_files, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Make checkpoint dir\n",
    "    training_model_dir = os.path.join(os.getcwd(), 'TrainingModel1')\n",
    "    os.makedirs(training_model_dir, exist_ok=True)\n",
    "    print(f\"Checkpoints will be saved to: {training_model_dir}\")\n",
    "\n",
    "    metrics_path = os.path.join(training_model_dir, \"training_metrics.pt\")\n",
    "\n",
    "    # Load existing metrics if available (Ensures metrics continue from previous runs)\n",
    "    if os.path.exists(metrics_path):\n",
    "        saved_metrics = torch.load(metrics_path)\n",
    "        train_metrics = saved_metrics['train_metrics']\n",
    "        test_metrics = saved_metrics['test_metrics']\n",
    "        print(\"Loaded previous training metrics!\")\n",
    "    else:\n",
    "        train_metrics = {'epoch': [], 'mse': [], 'ci': [], 'pearson': []}\n",
    "        test_metrics = {'epoch': [], 'mse': [], 'ci': [], 'pearson': []}\n",
    "        print(\"Starting fresh metrics tracking.\")\n",
    "\n",
    "    # Infer input dims from one sample\n",
    "    sample0 = load_sample(os.path.join(sample_dir, train_files[0]))\n",
    "    mol_data0, pro_data0 = sample0[0], sample0[1]\n",
    "    num_features_mol = mol_data0.x.size(1)\n",
    "    num_features_pro = pro_data0.x.size(1)\n",
    "\n",
    "    # Initialize model\n",
    "    model = GNNNet(num_features_mol=num_features_mol,\n",
    "                   num_features_pro=num_features_pro).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    # Possibly resume from checkpoint\n",
    "    start_epoch = 1\n",
    "    existing_checkpoints = [\n",
    "        f for f in os.listdir(training_model_dir)\n",
    "        if f.endswith('.pt') and f.startswith('model_epoch')\n",
    "    ]\n",
    "    last_ckpt_path = None  # Track previous checkpoint for deletion\n",
    "\n",
    "    if existing_checkpoints:\n",
    "        latest_ckpt = max(existing_checkpoints, key=lambda x: int(x.split('_epoch')[1].split('.pt')[0]))\n",
    "        ckpt_path = os.path.join(training_model_dir, latest_ckpt)\n",
    "        print(f\"Loading checkpoint from {ckpt_path}\")\n",
    "        ckpt = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        loaded_epoch = ckpt['epoch']\n",
    "        start_epoch = loaded_epoch + 1\n",
    "        last_ckpt_path = ckpt_path  # Store last checkpoint path for deletion\n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "    else:\n",
    "        print(\"No existing checkpoint found; starting fresh.\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(files):\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        batch_eval_size = 200\n",
    "        for batch_samples in batch_loader(files, sample_dir, batch_eval_size):\n",
    "            mol_list, pro_list, tgt_list = [], [], []\n",
    "            for (md, pd, t) in batch_samples:\n",
    "                mol_list.append(md)\n",
    "                pro_list.append(pd)\n",
    "                tgt_list.append(t)\n",
    "\n",
    "            mol_batch = Batch.from_data_list(mol_list).to(device)\n",
    "            pro_batch = Batch.from_data_list(pro_list).to(device)\n",
    "            t_tensor = torch.tensor(tgt_list, dtype=torch.float32, device=device)\n",
    "\n",
    "            out = model(mol_batch, pro_batch).view(-1)\n",
    "            all_preds.append(out)\n",
    "            all_targets.append(t_tensor)\n",
    "\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "        mse_val = mse_torch(all_preds, all_targets)\n",
    "        ci_val = ci_vectorized(all_preds, all_targets)\n",
    "        pearson_val = pearson_torch(all_preds, all_targets)\n",
    "        return mse_val, ci_val, pearson_val\n",
    "\n",
    "    # Training loop\n",
    "    batch_size = 30\n",
    "    for epoch in tqdm(range(start_epoch, num_epochs + 1), desc=\"Training\", unit=\"epoch\"):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch_samples in batch_loader(train_files, sample_dir, batch_size):\n",
    "            mol_list, pro_list, tgt_list = [], [], []\n",
    "            for (md, pd, t) in batch_samples:\n",
    "                mol_list.append(md)\n",
    "                pro_list.append(pd)\n",
    "                tgt_list.append(t)\n",
    "\n",
    "            mol_batch = Batch.from_data_list(mol_list).to(device)\n",
    "            pro_batch = Batch.from_data_list(pro_list).to(device)\n",
    "            t_tensor = torch.tensor(tgt_list, dtype=torch.float32, device=device).view(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(mol_batch, pro_batch).view(-1)\n",
    "            loss = loss_fn(out, t_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * len(batch_samples)\n",
    "\n",
    "        avg_loss = running_loss / len(train_files)\n",
    "        tqdm.write(f\"[Epoch {epoch}/{num_epochs}] Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Evaluate on train & test\n",
    "        train_mse, train_ci, train_pearson = evaluate(train_files)\n",
    "        test_mse, test_ci, test_pearson = evaluate(test_files)\n",
    "\n",
    "        train_metrics['epoch'].append(epoch)\n",
    "        train_metrics['mse'].append(train_mse)\n",
    "        train_metrics['ci'].append(train_ci)\n",
    "        train_metrics['pearson'].append(train_pearson)\n",
    "\n",
    "        test_metrics['epoch'].append(epoch)\n",
    "        test_metrics['mse'].append(test_mse)\n",
    "        test_metrics['ci'].append(test_ci)\n",
    "        test_metrics['pearson'].append(test_pearson)\n",
    "\n",
    "        tqdm.write(f\"  Train => MSE={train_mse:.4f}, CI={train_ci:.4f}, Pearson={train_pearson:.4f}\")\n",
    "        tqdm.write(f\"  Test  => MSE={test_mse:.4f}, CI={test_ci:.4f}, Pearson={test_pearson:.4f}\")\n",
    "\n",
    "        # Save new checkpoint\n",
    "        ckpt_name = f\"model_epoch{epoch}.pt\"\n",
    "        ckpt_path = os.path.join(training_model_dir, ckpt_name)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, ckpt_path)\n",
    "        tqdm.write(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "        # Delete the previous checkpoint after saving the new one\n",
    "        if last_ckpt_path and os.path.exists(last_ckpt_path):\n",
    "            os.remove(last_ckpt_path)\n",
    "            tqdm.write(f\"Deleted previous checkpoint: {last_ckpt_path}\")\n",
    "\n",
    "        # Update last checkpoint path\n",
    "        last_ckpt_path = ckpt_path\n",
    "\n",
    "        # Save/Update the metrics after each epoch\n",
    "        torch.save({\n",
    "            'train_metrics': train_metrics,\n",
    "            'test_metrics': test_metrics\n",
    "        }, metrics_path)\n",
    "\n",
    "    # Final evaluations\n",
    "    final_train_mse, final_train_ci, final_train_pearson = evaluate(train_files)\n",
    "    final_test_mse, final_test_ci, final_test_pearson = evaluate(test_files)\n",
    "    print(f\"\\nFinal Train => MSE={final_train_mse:.4f}, CI={final_train_ci:.4f}, Pearson={final_train_pearson:.4f}\")\n",
    "    print(f\"Final Test  => MSE={final_test_mse:.4f}, CI={final_test_ci:.4f}, Pearson={final_test_pearson:.4f}\")\n",
    "\n",
    "    # Save final metrics\n",
    "    torch.save({\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics\n",
    "    }, metrics_path)\n",
    "    print(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "    return train_metrics, test_metrics\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#               5. OPTIONAL: PLOT THE SAVED METRICS\n",
    "##############################################################################\n",
    "\n",
    "def plot_metrics(checkpoint_dir='TrainingModel1'):\n",
    "    \"\"\"\n",
    "    Load training_metrics.pt from the checkpoint_dir and plot MSE, CI, Pearson over epochs.\n",
    "    \"\"\"\n",
    "    metrics_path = os.path.join(checkpoint_dir, \"training_metrics.pt\")\n",
    "    if not os.path.exists(metrics_path):\n",
    "        print(f\"No metrics file found at {metrics_path}!\")\n",
    "        return\n",
    "\n",
    "    saved_data = torch.load(metrics_path)\n",
    "    train_metrics = saved_data['train_metrics']\n",
    "    test_metrics = saved_data['test_metrics']\n",
    "    epochs = train_metrics['epoch']\n",
    "\n",
    "    # Plot MSE\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, train_metrics['mse'], 'o-', label='Train MSE')\n",
    "    plt.plot(epochs, test_metrics['mse'], 'o-', label='Test MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title('Mean Squared Error over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(checkpoint_dir, \"MSE_plot.png\"))  \n",
    "    plt.show()\n",
    "\n",
    "    # Plot CI\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, train_metrics['ci'], 'o-', label='Train CI')\n",
    "    plt.plot(epochs, test_metrics['ci'], 'o-', label='Test CI')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Concordance Index')\n",
    "    plt.title('CI over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(checkpoint_dir, \"CI_plot.png\"))  \n",
    "    plt.show()\n",
    "\n",
    "    # Plot Pearson\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, train_metrics['pearson'], 'o-', label='Train Pearson')\n",
    "    plt.plot(epochs, test_metrics['pearson'], 'o-', label='Test Pearson')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Pearson Correlation')\n",
    "    plt.title('Pearson Correlation over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(checkpoint_dir, \"PEARSON_plot.png\"))  \n",
    "    plt.show()\n",
    "\n",
    "##############################################################################\n",
    "#                                 MAIN\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Adjust the paths/parameters as needed\n",
    "    SAMPLE_DIR = \"prepared_samples\"   # Directory with your .pt samples\n",
    "    NUM_EPOCHS = 250\n",
    "    TEST_SPLIT = 0.2\n",
    "    LR = 0.001\n",
    "\n",
    "    # 1) Train and evaluate\n",
    "    train_metrics, test_metrics = train_and_evaluate(\n",
    "        sample_dir=SAMPLE_DIR,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        test_size=TEST_SPLIT,\n",
    "        lr=LR\n",
    "    )\n",
    "\n",
    "    # 2) Plot the metrics\n",
    "    plot_metrics('TrainingModel1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.207291,
     "end_time": "2025-02-02T20:48:39.779250",
     "exception": false,
     "start_time": "2025-02-02T20:48:39.571959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6585052,
     "sourceId": 10635698,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6590451,
     "sourceId": 10643825,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6590542,
     "sourceId": 10643954,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15154.00795,
   "end_time": "2025-02-02T20:48:42.835865",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-02T16:36:08.827915",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
