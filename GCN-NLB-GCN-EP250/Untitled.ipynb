{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56c09213-6ae9-4f40-9e21-4b0891efe5d0",
   "metadata": {},
   "source": [
    "\\documentclass{article}\r\n",
    "\\usepackage{amsmath, amssymb, amsfonts}\r\n",
    "\\usepackage{booktabs}\r\n",
    "\\usepackage{array}\r\n",
    "\\usepackage{geometry}\r\n",
    "\\geometry{margin=1in}\r\n",
    "\r\n",
    "\\begin{document}\r\n",
    "\r\n",
    "\\section*{Cross-Attention Mechanism: Detailed Explanation and Example}\r\n",
    "\r\n",
    "\\subsection*{1. General Formulation}\r\n",
    "\r\n",
    "Suppose we have two graphs:\r\n",
    "\\begin{itemize}\r\n",
    "    \\item Graph A with node features $X_A \\in \\mathbb{R}^{N_A \\times d}$.\r\n",
    "    \\item Graph B with node features $X_B \\in \\mathbb{R}^{N_B \\times d}$.\r\n",
    "\\end{itemize}\r\n",
    "\r\n",
    "For the cross-attention from Graph A to Graph B, we compute:\r\n",
    "\\begin{enumerate}\r\n",
    "    \\item \\textbf{Queries, Keys, and Values:}\r\n",
    "    \\[\r\n",
    "    Q_A = X_A W_{Q_A}, \\quad K_B = X_B W_{K_A}, \\quad V_B = X_B W_{V_A},\r\n",
    "    \\]\r\n",
    "    where $W_{Q_A}$, $W_{K_A}$, and $W_{V_A}$ are learned weight matrices in $\\mathbb{R}^{d \\times d}$.\r\n",
    "    \r\n",
    "    \\item \\textbf{Attention Scores:} Compute the scaled dot-product\r\n",
    "    \\[\r\n",
    "    S = \\frac{Q_A K_B^T}{\\sqrt{d}} \\quad \\in \\mathbb{R}^{N_A \\times N_B}.\r\n",
    "    \\]\r\n",
    "    \r\n",
    "    \\item \\textbf{Softmax:} For each row $i$, compute\r\n",
    "    \\[\r\n",
    "    A_{ij} = \\frac{\\exp(S_{ij})}{\\sum_{k=1}^{N_B} \\exp(S_{ik})}.\r\n",
    "    \\]\r\n",
    "    \r\n",
    "    \\item \\textbf{Updated Features:}\r\n",
    "    \\[\r\n",
    "    Z_A = A V_B, \\quad \\text{or element-wise, } Z_A[i] = \\sum_{j=1}^{N_B} A_{ij} \\, V_B[j].\r\n",
    "    \\]\r\n",
    "\\end{enumerate}\r\n",
    "\r\n",
    "The reverse direction (Graph B $\\rightarrow$ Graph A) is computed analogously.\r\n",
    "\r\n",
    "\\subsection*{2. A Simple Numeric Example}\r\n",
    "\r\n",
    "Let:\r\n",
    "\\[\r\n",
    "X_A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}, \\quad\r\n",
    "X_B = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}.\r\n",
    "\\]\r\n",
    "Assume $d = 2$ and for simplicity, let $W_{Q_A}=W_{K_A}=W_{V_A}=I$ (the identity matrix). Then,\r\n",
    "\\[\r\n",
    "Q_A = X_A,\\quad K_B = X_B,\\quad V_B = X_B.\r\n",
    "\\]\r\n",
    "\r\n",
    "\\paragraph{Step 1: Compute the Scaled Dot-Product.}\r\n",
    "\r\n",
    "The scaling factor is $\\sqrt{2} \\approx 1.414$. Compute $S = Q_A K_B^T / \\sqrt{2}$.\r\n",
    "\r\n",
    "For node 1 in Graph A ($[1,2]$):\r\n",
    "\\[\r\n",
    "\\begin{aligned}\r\n",
    "S_{11} &= \\frac{1\\cdot 1 + 2\\cdot 0}{1.414} \\approx 0.7071,\\\\[1mm]\r\n",
    "S_{12} &= \\frac{1\\cdot 0 + 2\\cdot 1}{1.414} \\approx 1.4142,\\\\[1mm]\r\n",
    "S_{13} &= \\frac{1\\cdot 1 + 2\\cdot 1}{1.414} \\approx 2.1213.\r\n",
    "\\end{aligned}\r\n",
    "\\]\r\n",
    "Thus, the first row of $S$ is approximately $[0.7071,\\; 1.4142,\\; 2.1213]$.\r\n",
    "\r\n",
    "For node 2 in Graph A ($[3,4]$):\r\n",
    "\\[\r\n",
    "\\begin{aligned}\r\n",
    "S_{21} &= \\frac{3\\cdot 1 + 4\\cdot 0}{1.414} \\approx 2.1213,\\\\[1mm]\r\n",
    "S_{22} &= \\frac{3\\cdot 0 + 4\\cdot 1}{1.414} \\approx 2.8284,\\\\[1mm]\r\n",
    "S_{23} &= \\frac{3\\cdot 1 + 4\\cdot 1}{1.414} \\approx 4.9497.\r\n",
    "\\end{aligned}\r\n",
    "\\]\r\n",
    "For node 3 in Graph A ($[5,6]$):\r\n",
    "\\[\r\n",
    "\\begin{aligned}\r\n",
    "S_{31} &= \\frac{5\\cdot 1 + 6\\cdot 0}{1.414} \\approx 3.5355,\\\\[1mm]\r\n",
    "S_{32} &= \\frac{5\\cdot 0 + 6\\cdot 1}{1.414} \\approx 4.2426,\\\\[1mm]\r\n",
    "S_{33} &= \\frac{5\\cdot 1 + 6\\cdot 1}{1.414} \\approx 7.7782.\r\n",
    "\\end{aligned}\r\n",
    "\\]\r\n",
    "\r\n",
    "\\paragraph{Step 2: Apply Softmax to Each Row.}\r\n",
    "\r\n",
    "For the first row:\r\n",
    "\\[\r\n",
    "\\exp(0.7071)\\approx 2.028,\\quad \\exp(1.4142)\\approx 4.113,\\quad \\exp(2.1213)\\approx 8.338.\r\n",
    "\\]\r\n",
    "Sum $\\approx 14.479$. Thus, the softmax weights are approximately:\r\n",
    "\\[\r\n",
    "[0.14,\\; 0.284,\\; 0.575].\r\n",
    "\\]\r\n",
    "\r\n",
    "Similarly, you compute the softmax for rows 2 and 3 (approximations):\r\n",
    "\\[\r\n",
    "A_2 \\approx [0.05,\\; 0.102,\\; 0.848],\\quad\r\n",
    "A_3 \\approx [0.0137,\\; 0.0278,\\; 0.9585].\r\n",
    "\\]\r\n",
    "\r\n",
    "\\paragraph{Step 3: Compute the Updated Features.}\r\n",
    "\r\n",
    "Recall $V_B = X_B = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$.\r\n",
    "\r\n",
    "For node 1 in Graph A:\r\n",
    "\\[\r\n",
    "Z_A[1] \\approx 0.14\\,[1,0] + 0.284\\,[0,1] + 0.575\\,[1,1] = [0.14+0+0.575,\\; 0+0.284+0.575] \\approx [0.715,\\; 0.859].\r\n",
    "\\]\r\n",
    "\r\n",
    "For node 2:\r\n",
    "\\[\r\n",
    "Z_A[2] \\approx 0.05\\,[1,0] + 0.102\\,[0,1] + 0.848\\,[1,1] \\approx [0.05+0+0.848,\\; 0+0.102+0.848] \\approx [0.898,\\; 0.950].\r\n",
    "\\]\r\n",
    "\r\n",
    "For node 3:\r\n",
    "\\[\r\n",
    "Z_A[3] \\approx 0.0137\\,[1,0] + 0.0278\\,[0,1] + 0.9585\\,[1,1] \\approx [0.0137+0+0.9585,\\; 0+0.0278+0.9585] \\approx [0.9722,\\; 0.9863].\r\n",
    "\\]\r\n",
    "\r\n",
    "Thus, the updated feature matrix for Graph A is approximately:\r\n",
    "\\[\r\n",
    "Z_A \\approx \\begin{bmatrix}\r\n",
    "0.715 & 0.859 \\\\\r\n",
    "0.898 & 0.950 \\\\\r\n",
    "0.972 & 0.986 \r\n",
    "\\end{bmatrix}.\r\n",
    "\\]\r\n",
    "\r\n",
    "\\subsection*{3. Summary}\r\n",
    "\r\n",
    "The cross-attention mechanism allows Graph A to update its node features by computing attention over the nodes of Graph B. The key formulas are:\r\n",
    "\\[\r\n",
    "\\begin{aligned}\r\n",
    "Q_A &= X_A W_{Q_A},\\\\[1mm]\r\n",
    "K_B &= X_B W_{K_A},\\\\[1mm]\r\n",
    "V_B &= X_B W_{V_A},\\\\[1mm]\r\n",
    "S &= \\frac{Q_A K_B^T}{\\sqrt{d}},\\\\[1mm]\r\n",
    "A_{ij} &= \\frac{\\exp(S_{ij})}{\\sum_{k=1}^{N_B} \\exp(S_{ik})},\\\\[1mm]\r\n",
    "Z_A &= A\\, V_B.\r\n",
    "\\end{aligned}\r\n",
    "\\]\r\n",
    "\r\n",
    "A similar process is applied for Graph B $\\rightarrow$ Graph A.\r\n",
    "\r\n",
    "\\end{document}\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e52c4-dbdf-4c2a-9f39-9f95f5f291c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
